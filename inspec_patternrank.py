# -*- coding: utf-8 -*-
"""inspec_PatternRank.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13d6fecgUsNYw5ItLHk1GZOuy0kxpCzuu
"""

!pip install datasets
!pip install transformers
!pip install keyphrase-vectorizers
!pip install keybert

from datasets import load_dataset
from transformers import AutoTokenizer
from keybert import KeyBERT
from keyphrase_vectorizers import KeyphraseCountVectorizer
import string
import nltk
nltk.download('punkt')
from sklearn.metrics import classification_report,confusion_matrix,ConfusionMatrixDisplay
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
from nltk.corpus import stopwords
import matplotlib.pyplot as plt

kw_model = KeyBERT()
# Labels
label_list = ["B", "I", "O"]
lbl2idx = {"B": 0, "I": 1, "O": 2}
idx2label = {0: "B", 1: "I", 2: "O"}

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
max_length = 512

# Dataset parameters
dataset_full_name = "midas/inspec"
dataset_subset = "raw"
dataset_document_column = "document"
dataset_biotags_column = "doc_bio_tags"

def preprocess_fuction(all_samples_per_split):
    tokenized_samples = tokenizer.batch_encode_plus(
        all_samples_per_split[dataset_document_column],
        padding="max_length",
        truncation=True,
        is_split_into_words=True,
        max_length=max_length,
    )
    total_adjusted_labels = []
    for k in range(0, len(tokenized_samples["input_ids"])):
        prev_wid = -1
        word_ids_list = tokenized_samples.word_ids(batch_index=k)
        existing_label_ids = all_samples_per_split[dataset_biotags_column][k]
        i = -1
        adjusted_label_ids = []

        for wid in word_ids_list:
            if wid is None:
                adjusted_label_ids.append(lbl2idx["O"])
            elif wid != prev_wid:
                i = i + 1
                adjusted_label_ids.append(lbl2idx[existing_label_ids[i]])
                prev_wid = wid
            else:
                adjusted_label_ids.append(
                    lbl2idx[
                        f"{'I' if existing_label_ids[i] == 'B' else existing_label_ids[i]}"
                    ]
                )

        total_adjusted_labels.append(adjusted_label_ids)
    tokenized_samples["labels"] = total_adjusted_labels
    return tokenized_samples

# Load dataset
dataset = load_dataset(dataset_full_name, dataset_subset)

# Preprocess dataset
tokenized_dataset = dataset.map(preprocess_fuction, batched=True)

train=tokenized_dataset['train']
val=tokenized_dataset['validation']
test=tokenized_dataset['test']

train_doc=train['document']

train_tags=train['doc_bio_tags']


test_doc=test['document']
test_tags=test['doc_bio_tags']


def tag_bio(text,ann):
    
    tags=['O']*len(text)
    last_i=0
    for key in ann:
        
        key_phrase=key[0].split()
        for i in range(last_i,len(text)):
            
            if tags[i]=='O':
                c=0
                for count in range(len(key_phrase)):
                    if i+count<len(text):
                        if key_phrase[count]!= text[i+count]:
                            break
                        else:
                            c=c+1
                
                if c == len(key_phrase):
                    tags[i]='B'
                    for k in range(1,len(key_phrase)):
                        tags[i+k]='I'
                    last_i=i+c 
    return tags
        

def get_lower(text,ann):
    
    return [[j.lower() for j in i] for i in ann],[i.lower() for i in text]

def preprocess (text,ann):
    return tag_bio(text,ann)

key_words=kw_model.extract_keywords(top_n=10,docs=[" ".join(i) for i in train_doc], vectorizer=KeyphraseCountVectorizer(),stop_words=[])
y_pred,y_true=[],[]
for i in range(len(train_doc)):
    tag=preprocess (train_doc[i],key_words[i])
    y_pred+=tag
    y_true+=train_tags[i]
print(classification_report(y_true,y_pred))  
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_true,y_pred)).plot()



key_words=kw_model.extract_keywords(top_n=10,docs=[" ".join(i) for i in test_doc], vectorizer=KeyphraseCountVectorizer(),stop_words=[])
y_pred,y_true=[],[]
for i in range(len(test_doc)):
    tag=preprocess (test_doc[i],key_words[i])
    y_pred+=tag
    y_true+=test_tags[i]
print(classification_report(y_true,y_pred))



