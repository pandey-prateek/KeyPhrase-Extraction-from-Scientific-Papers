# -*- coding: utf-8 -*-
"""multihead_attn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PTDGGtsBT8wVJi_W9yVlcUeJtVbo6jXq
"""

!pip install datasets
!pip install transformers

import datasets as ds
from datasets import load_dataset
from transformers import AutoTokenizer

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained("bloomberg/KeyBART", add_prefix_space=True)

# Dataset parameters
dataset_full_name = "midas/inspec"
dataset_subset = "raw"
dataset_document_column = "document"

keyphrase_sep_token = ";"

def preprocess_keyphrases(text_ids, kp_list):
    kp_order_list = []
    kp_set = set(kp_list)
    text = tokenizer.decode(
        text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True
    )
    text = text.lower()
    for kp in kp_set:
        kp = kp.strip()
        kp_index = text.find(kp.lower())
        kp_order_list.append((kp_index, kp))

    kp_order_list.sort()
    present_kp, absent_kp = [], []

    for kp_index, kp in kp_order_list:
        if kp_index < 0:
            absent_kp.append(kp)
        else:
            present_kp.append(kp)
    return present_kp, absent_kp


def preprocess_fuction(samples):
    processed_samples = {"input_ids": [], "attention_mask": [], "labels": []}
    for i, sample in enumerate(samples[dataset_document_column]):
        input_text = " ".join(sample)
        inputs = tokenizer(
            input_text,
            padding="max_length",
            truncation=True,
        )
        present_kp, absent_kp = preprocess_keyphrases(
            text_ids=inputs["input_ids"],
            kp_list=samples["extractive_keyphrases"][i]
            + samples["abstractive_keyphrases"][i],
        )
        keyphrases = present_kp
        keyphrases += absent_kp

        target_text = f" {keyphrase_sep_token} ".join(keyphrases)

        with tokenizer.as_target_tokenizer():
            targets = tokenizer(
                target_text, max_length=40, padding="max_length", truncation=True
            )
            targets["input_ids"] = [
                (t if t != tokenizer.pad_token_id else -100)
                for t in targets["input_ids"]
            ]
        for key in inputs.keys():
            processed_samples[key].append(inputs[key])
        processed_samples["labels"].append(targets["input_ids"])
    return processed_samples

# Load dataset
dataset = load_dataset(dataset_full_name, dataset_subset)
# Preprocess dataset
tokenized_dataset = dataset.map(preprocess_fuction, batched=True)

import torch
import torch.nn as nn
from torchtext.vocab import build_vocab_from_iterator
train=tokenized_dataset['train']
test=tokenized_dataset['test']
val=tokenized_dataset['validation']

train_document=train['document']
train_keyphrase=train['extractive_keyphrases']
train_tags=train['doc_bio_tags']


test_document=test['document']
test_keyphrase=test['extractive_keyphrases']
test_tags=test['doc_bio_tags']

val_document=val['document']
val_keyphrase=val['extractive_keyphrases']
val_tags=val['doc_bio_tags']
def yield_tokens(tokenlist):
    for tokens in tokenlist:
        yield tokens
def get_data(dataset, vocab):
    data = []
    for sentence in dataset:
            tokens = [vocab[token] for token in sentence]
            data.append(torch.LongTensor(tokens))
    return data
train_vocab=build_vocab_from_iterator(yield_tokens(train_document),min_freq=1,specials=["<UNK>"])
train_vocab.set_default_index(0)
data_train = get_data(train_document,train_vocab)
tag_vocab=build_vocab_from_iterator(yield_tokens(train_tags),min_freq=1)
data_tag=get_data(train_tags,tag_vocab)


data_val = get_data(val_document,train_vocab)
data_val_tag=get_data(val_tags,tag_vocab)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import math
class PositionalEmbedding(nn.Module):
    def __init__(self,max_seq_len,embed_model_dim):
        """
        Args:
            seq_len: length of input sequence
            embed_model_dim: demension of embedding
        """
        super(PositionalEmbedding, self).__init__()
        self.embed_dim = embed_model_dim

        pe = torch.zeros(max_seq_len,self.embed_dim)
        for pos in range(max_seq_len):
            for i in range(0,self.embed_dim,2):
                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/self.embed_dim)))
                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)


    def forward(self, x):
        """
        Args:
            x: input vector
        Returns:
            x: output
        """
      
        # make embeddings relatively larger
        x = x * math.sqrt(self.embed_dim)
        #add constant to embedding
        seq_len = x.size(1)
        x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)
        return x        
class MultiHeadAttention(nn.Module):
    def __init__(self, embed_dim,n_heads):
        """
        Args:
            embed_dim: dimension of embeding vector output
            n_heads: number of self attention heads
        """
        super(MultiHeadAttention, self).__init__()

        self.embed_dim = embed_dim    #512 dim
        self.n_heads = n_heads   #8
        self.single_head_dim = int(self.embed_dim / self.n_heads)   #512/8 = 64  . each key,query, value will be of 64d
       
        #key,query and value matrixes    #64 x 64   
        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False)  # single key matrix for all 8 keys #512x512
        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False)
        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False)
        self.out = nn.Linear(self.n_heads*self.single_head_dim ,self.embed_dim) 

    def forward(self,key,query,value,mask=None):    #batch_size x sequence_length x embedding_dim    # 32 x 10 x 512
        
        """
        Args:
           key : key vector
           query : query vector
           value : value vector
           mask: mask for decoder
        
        Returns:
           output vector from multihead attention
        """
        batch_size = key.size(0)
        seq_length = key.size(1)
        
        # query dimension can change in decoder during inference. 
        # so we cant take general seq_length
        seq_length_query = query.size(1)
        
        # 32x10x512
        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim)  #batch_size x sequence_length x n_heads x single_head_dim = (32x10x8x64)
        query = query.view(batch_size, seq_length_query, self.n_heads, self.single_head_dim) #(32x10x8x64)
        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim) #(32x10x8x64)
        k = self.key_matrix(key)       # (32x10x8x64)
        q = self.query_matrix(query)   
        v = self.value_matrix(value)
        
        q = q.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)    # (32 x 8 x 10 x 64)
        k = k.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)
        v = v.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)
       
        # computes attention
        # adjust key for matrix multiplication
        k_adjusted = k.transpose(-1,-2)  #(batch_size, n_heads, single_head_dim, seq_ken)  #(32 x 8 x 64 x 10)
        product = torch.matmul(q, k_adjusted)  #(32 x 8 x 10 x 64) x (32 x 8 x 64 x 10) = #(32x8x10x10)
      
        
        # fill those positions of product matrix as (-1e20) where mask positions are 0
        if mask is not None:
             product = product.masked_fill(mask == 0, float("-1e20"))

        #divising by square root of key dimension
        product = product / math.sqrt(self.single_head_dim) # / sqrt(64)

        #applying softmax
        scores = F.softmax(product, dim=-1)
        
        #mutiply with value matrix
        scores = torch.matmul(scores, v)  ##(32x8x 10x 10) x (32 x 8 x 10 x 64) = (32 x 8 x 10 x 64) 
        
        #concatenated output
        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length_query, self.single_head_dim*self.n_heads)  # (32x8x10x64) -> (32x10x8x64)  -> (32,10,512)
        
        output = self.out(concat) #(32,10,512) -> (32,10,512)
        return output
class attbilstm(nn.Module):
    def __init__(self,hidden_dim,emb_dim,vocab_size,tag_size,n_layers=1,max_seq_len=1000):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.emb_dim = emb_dim

        self.positionalencoding = PositionalEmbedding(max_seq_len,hidden_dim*2)
        self.embedding = nn.Embedding(vocab_size,emb_dim)
        self.attention = MultiHeadAttention(hidden_dim*2,8)
        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=n_layers, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(hidden_dim*2,tag_size)
        self.dropout = nn.Dropout(0.5)
        #self.hidden = nn.Parameters(self.batch_size, self.hidden_dim)
    
    def attnetwork(self, encoder_out, final_hidden):
        hidden = final_hidden.squeeze(0)
        #M = torch.tanh(encoder_out)
        attn_weights = torch.bmm(encoder_out, hidden.unsqueeze(2)).squeeze(2)
        soft_attn_weights = F.softmax(attn_weights, 1)
        new_hidden = torch.bmm(encoder_out.transpose(1,2), soft_attn_weights.unsqueeze(2)).squeeze(2)
        #print (wt.shape, new_hidden.shape)
        #new_hidden = torch.tanh(new_hidden)
        #print ('UP:', new_hidden, new_hidden.shape)
        
        return new_hidden
    
    def forward(self, sequence):
        emb_input = self.embedding(sequence)
        inputx = self.dropout(emb_input)
        output, (hn, cn) = self.encoder(inputx)
        out=self.positionalencoding(output)
        # fbout = output[:, :, :self.hidden_dim]+ output[:, :, self.hidden_dim:] #sum bidir outputs F+B
        # fbout = fbout.permute(1,0,2)
        # fbhn = (hn[-2,:,:]+hn[-1,:,:]).unsqueeze(0)
        # #print (fbhn.shape, fbout.shape)
        # attn_out = self.attnetwork(fbout, fbhn)
        attn_out = self.attention(out,out,out)
        #attn1_out = self.attnetwork1(output, hn)
        
        logits = self.fc(attn_out)
        return logits.view(-1,3)

from torch import optim
from tqdm import tqdm
from sklearn.metrics import classification_report
EMBEDDING_DIM = 256
HIDDEN_DIM = 256
EPOCHS = 1
LEARNING_RATE=0.0001
NUMBER_OF_LAYERS=2
model = attbilstm(HIDDEN_DIM,EMBEDDING_DIM,train_vocab.__len__(), tag_vocab.__len__(),NUMBER_OF_LAYERS)
loss_function = nn.NLLLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

print("EMBEDDING DIMENSION",EMBEDDING_DIM)
print("HIDDEN DIMENSION",HIDDEN_DIM)
print("EPOCHS",EPOCHS)

print("LEARNING RATE",LEARNING_RATE)
print("NUMBER OF LAYERS",NUMBER_OF_LAYERS)

for epoch in range(EPOCHS):
    l=0
    acc=0
    tags=[]
    y_true=[]
    for i in tqdm(range(len(train_document))):
        model.zero_grad()
        tag_scores = model(data_train[i].view(1,-1))
        
        loss = loss_function(tag_scores, data_tag[i])
        indices = torch.max(tag_scores, 1)[1]
        tags+=list(indices.detach().numpy())
        y_true+=list(data_tag[i].detach().numpy())
        loss.backward()
        optimizer.step()
    print(classification_report(y_true,tags))
with torch.no_grad():
      model.eval()
      tags=[]
      y_true=[]
      for i in tqdm(range(len(val_document))):
          tag_scores = model(data_val[i].view(1,-1))
          loss = loss_function(tag_scores, data_val_tag[i])
          indices = torch.max(tag_scores, 1)[1]
          tags+=list(indices.detach().numpy())
          y_true+=list(data_val_tag[i].detach().numpy())
          
      print(classification_report(y_true,tags))

torch.save(model.state_dict(), 'multi_head_attn.pt')

print(model)

